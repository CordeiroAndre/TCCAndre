// Georgy Treshchev 2023.

#include "SpeechRecognizerThread.h"

#include "SpeechRecognizerDefines.h"
#include "SpeechRecognizerTypes.h"

#include "AudioResampler.h"
#include "HAL/RunnableThread.h"
#include "SampleBuffer.h"
#include "SpeechRecognizerModel.h"

#include "Async/Async.h"
#include "AudioThread.h"
#include "SpeechRecognizerSettings.h"

#include "SpeechRecognizerPrivate.h"

/**
 * Called when a new text segment is generated by the whisper
 *
 * @param WhisperContext The whisper context associated with the text segment
 * @param WhisperState The whisper state associated with the text segment
 * @param NewSegmentCount The number of new text segments that have been generated
 * @param UserData User-defined data pointer, which should be a pointer to a WhisperSpeechRecognizerUserData struct
 */
void WhisperNewTextSegmentCallback(whisper_context* WhisperContext, whisper_state* WhisperState, int NewSegmentCount, void* UserData)
{
	if (!WhisperContext || !WhisperState || !UserData)
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Warning, TEXT("Invalid parameters passed to the whisper new text segment callback"));
		return;
	}

	TSharedPtr<FSpeechRecognizerThread> SpeechRecognizerSharedPtr = static_cast<FWhisperSpeechRecognizerUserData*>(UserData)->SpeechRecognizerWeakPtr.Pin();
	if (!SpeechRecognizerSharedPtr.IsValid())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Warning, TEXT("Speech recognizer is not valid, text segment will not be processed"));
		return;
	}

	// We should still process the text segment even if the speech recognizer is stopped
	/*if (SpeechRecognizerSharedPtr->GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Warning, TEXT("Speech recognizer is stopped, text segment will not be processed"));
		return;
	}*/

	const int32 TotalSegmentCount = whisper_full_n_segments(WhisperContext);
	const int32 StartIndex = TotalSegmentCount - NewSegmentCount;

	for (int32 Index = StartIndex; Index < TotalSegmentCount; ++Index)
	{
		const char* TextPerSegment = whisper_full_get_segment_text(WhisperContext, static_cast<int>(Index));
		FString TextPerSegment_String = UTF8_TO_TCHAR(TextPerSegment);

		AsyncTask(ENamedThreads::GameThread, [SpeechRecognizerSharedPtr, TextPerSegment_String = MoveTemp(TextPerSegment_String)]() mutable
		{
			UE_LOG(LogRuntimeSpeechRecognizer, Log, TEXT("Recognized text segment: \"%s\""), *TextPerSegment_String);
			SpeechRecognizerSharedPtr->OnRecognizedTextSegment.ExecuteIfBound(TextPerSegment_String);
		});
	}
}

/**
 * Called every time the whisper encoder begins processing a new audio buffer. Implements the aborting mechanism for the whisper encoder
 *
 * @param WhisperContext The whisper context associated with the user data
 * @param WhisperState The whisper state associated with the user data
 * @param UserData User-defined data pointer, which should be a pointer to a WhisperSpeechRecognizerUserData struct
 */
bool WhisperEncoderBeginCallback(whisper_context* WhisperContext, whisper_state* WhisperState, void* UserData)
{
	if (!WhisperContext || !WhisperState || !UserData)
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Warning, TEXT("Aborting whisper recognition due to invalid context or user data"));
		return false;
	}

	TSharedPtr<FSpeechRecognizerThread> SpeechRecognizerSharedPtr = static_cast<FWhisperSpeechRecognizerUserData*>(UserData)->SpeechRecognizerWeakPtr.Pin();
	if (!SpeechRecognizerSharedPtr.IsValid() || SpeechRecognizerSharedPtr->GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Warning, TEXT("Aborting whisper recognition due to stop request"));
		return false;
	}

	return true;
}

FWhisperSpeechRecognizerState::FWhisperSpeechRecognizerState()
	: WhisperContext(nullptr), WhisperParameters(nullptr)
{
}

bool FWhisperSpeechRecognizerState::Init(uint8* BulkDataPtr, int64 BulkDataSize, TSharedPtr<FSpeechRecognizerThread> SpeechRecognizerPtr)
{
	WhisperContext = whisper_init_from_buffer(BulkDataPtr, BulkDataSize);
	if (!WhisperContext)
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Failed to create whisper context from buffer"));
		return false;
	}

	WhisperParameters = new whisper_full_params(whisper_full_default_params(whisper_sampling_strategy::WHISPER_SAMPLING_GREEDY));
	if (!WhisperParameters)
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Failed to create whisper parameters"));
		return false;
	}

	WhisperUserData = FWhisperSpeechRecognizerUserData{SpeechRecognizerPtr};
	return true;
}

void FWhisperSpeechRecognizerState::Release()
{
	if (WhisperContext)
	{
		whisper_free(WhisperContext);
		WhisperContext = nullptr;
	}

	if (WhisperParameters)
	{
		delete WhisperParameters;
		WhisperParameters = nullptr;
	}

	WhisperUserData = FWhisperSpeechRecognizerUserData();
}

void FSpeechRecognitionParameters::SetNonStreamingDefaults()
{
	// These are the default values for the whisper cpp library
	bNoContext = false;
	bSingleSegment = false;
	MaxTokens = 0;
	AudioContextSize = 0;
	TemperatureToIncrease = 0.2f;
}

void FSpeechRecognitionParameters::SetStreamingDefaults()
{
	// Taken from https://github.com/ggerganov/whisper.cpp/blob/master/examples/stream.wasm/emscripten.cpp
	bNoContext = true;
	bSingleSegment = true;
	MaxTokens = 32;
	AudioContextSize = 768;
	TemperatureToIncrease = -1.0f;
}

void FSpeechRecognitionParameters::FillWhisperStateParameters(FWhisperSpeechRecognizerState& WhisperState) const
{
	// Disable all prints
	{
		WhisperState.WhisperParameters->print_realtime = false;
		WhisperState.WhisperParameters->print_progress = false;
		WhisperState.WhisperParameters->print_timestamps = false;
		WhisperState.WhisperParameters->print_special = false;
	}

	WhisperState.WhisperParameters->translate = bTranslateToEnglish;

	WhisperState.WhisperParameters->no_context = bNoContext;
	WhisperState.WhisperParameters->single_segment = bSingleSegment;
	WhisperState.WhisperParameters->max_tokens = MaxTokens;
	WhisperState.WhisperParameters->audio_ctx = AudioContextSize;
	WhisperState.WhisperParameters->temperature_inc = TemperatureToIncrease;

	WhisperState.WhisperParameters->language = EnumToString(Language);
	WhisperState.WhisperParameters->n_threads = NumOfThreads > 0 ? NumOfThreads : (FPlatformProcess::SupportsMultithreading() ? FMath::Min(6, FPlatformMisc::NumberOfCoresIncludingHyperthreads()) : 1);

	WhisperState.WhisperParameters->speed_up = bSpeedUp;

	// Setting up the new segment callback, which is called on every new recognized text segment
	{
		WhisperState.WhisperParameters->new_segment_callback = WhisperNewTextSegmentCallback;
		WhisperState.WhisperParameters->new_segment_callback_user_data = &WhisperState.WhisperUserData;
	}

	// Setting up the abort mechanism callback, which is called before every encoder run
	{
		WhisperState.WhisperParameters->encoder_begin_callback = WhisperEncoderBeginCallback;
		WhisperState.WhisperParameters->encoder_begin_callback_user_data = &WhisperState.WhisperUserData;
	}
}

FSpeechRecognizerThread::FSpeechRecognizerThread()
	: bIsStopped(true), bIsFinished(true)
{
}

FSpeechRecognizerThread::~FSpeechRecognizerThread()
{
	StopThread();
}

bool FSpeechRecognizerThread::StartThread()
{
	if (!GetIsStopped())
	{
		const FString ShortErrorMessage = TEXT("Thread start failed");
		const FString LongErrorMessage = TEXT("Unable to start an already running thread");
		ReportError(ShortErrorMessage, LongErrorMessage);
		return false;
	}

	if (!DoesSharedInstanceExist())
	{
		const FString ShortErrorMessage = TEXT("Recognizer initialization failed");
		const FString LongErrorMessage = TEXT("Speech recognizer can only be initialized as being created as shared instance");
		ReportError(ShortErrorMessage, LongErrorMessage);
		return false;
	}

	TSharedPtr<FSpeechRecognizerThread> SharedThis = AsShared();
	if (!SharedThis.IsValid())
	{
		const FString ShortErrorMessage = TEXT("Failed to get shared instance");
		const FString LongErrorMessage = TEXT("Failed to get shared instance of speech recognizer");
		ReportError(ShortErrorMessage, LongErrorMessage);
		return false;
	}

	// Check if the language is valid
	{
		const char* LanguageString = EnumToString(RecognitionParameters.Language);
		if (RecognitionParameters.Language != ESpeechRecognizerLanguage::Auto && whisper_lang_id(LanguageString) == -1)
		{
			const FString ShortErrorMessage = TEXT("Language identification failed");
			const FString LongErrorMessage = FString::Printf(TEXT("Failed to identify the language '%hs'"), LanguageString);
			ReportError(ShortErrorMessage, LongErrorMessage);
			return false;
		}
	}

	uint8* ModelBulkDataPtr = nullptr;
	int64 ModelBulkDataSize = 0;
	if (!LoadLanguageModel(ModelBulkDataPtr, ModelBulkDataSize))
	{
		// Error message should have been reported by LoadLanguageModel
		return false;
	}

	if (!WhisperState.Init(ModelBulkDataPtr, ModelBulkDataSize, SharedThis))
	{
		const FString ShortErrorMessage = TEXT("Recognizer initialization failed");
		const FString LongErrorMessage = TEXT("Failed to initialize whisper from the language model");
		ReportError(ShortErrorMessage, LongErrorMessage);
		return false;
	}

	FMemory::Free(ModelBulkDataPtr);

	// Ensure that the language model supports the requested features
	{
		const bool bMultilingual = whisper_is_multilingual(WhisperState.WhisperContext) != 0;
		if (RecognitionParameters.Language == ESpeechRecognizerLanguage::Auto && !bMultilingual)
		{
			const FString ShortErrorMessage = TEXT("Automatic language detection failed");
			const FString LongErrorMessage = TEXT("The selected language model does not support multilingual recognition therefore automatic language detection is not possible");
			ReportError(ShortErrorMessage, LongErrorMessage);
			return false;
		}

		if (RecognitionParameters.bTranslateToEnglish && !bMultilingual)
		{
			const FString ShortErrorMessage = TEXT("Translation failed");
			const FString LongErrorMessage = TEXT("The selected language model does not support multilingual recognition therefore translation is not possible");
			ReportError(ShortErrorMessage, LongErrorMessage);
			return false;
		}
	}

	RecognitionParameters.FillWhisperStateParameters(WhisperState);
	bIsStopped.AtomicSet(false);
	bIsFinished.AtomicSet(true);

	FRunnableThread* ThreadPtr = FRunnableThread::Create(this, TEXT("SpeechRecognizerThread"), 0, TPri_Highest, FPlatformAffinity::GetTaskGraphHighPriorityTaskMask());
	if (!ThreadPtr)
	{
		const FString ShortErrorMessage = TEXT("Thread creation failed");
		const FString LongErrorMessage = TEXT("Failed to create the thread for the speech recognizer");
		ReportError(ShortErrorMessage, LongErrorMessage);
		return false;
	}

	Thread = TUniquePtr<FRunnableThread>(ThreadPtr);
	return true;
}

void FSpeechRecognizerThread::StopThread()
{
	Stop();

	if (Thread.IsValid())
	{
		Thread->WaitForCompletion();
		Thread.Reset();
	}

	bIsFinished.AtomicSet(true);
}

void FSpeechRecognizerThread::ProcessPCMData(Audio::FAlignedFloatBuffer PCMData, float SampleRate, uint32 NumOfChannels, bool bLast)
{
	if (GetIsStopped())
	{
		const FString ShortErrorMessage = TEXT("Audio processing failed");
		const FString LongErrorMessage = TEXT("The audio data could not be processed to the recognizer since the thread is stopped");
		ReportError(ShortErrorMessage, LongErrorMessage);
		return;
	}

	// Make sure to process the data in the audio thread
	if (!IsInAudioThread())
	{
		FAudioThread::RunCommandOnAudioThread([SpeechRecognizerSharedPtr = WhisperState.WhisperUserData.SpeechRecognizerWeakPtr.Pin(), PCMData = MoveTemp(PCMData), SampleRate, NumOfChannels, bLast]() mutable
		{
			if (!SpeechRecognizerSharedPtr.IsValid())
			{
				UE_LOG(LogRuntimeSpeechRecognizer, Warning, TEXT("Failed to process the audio data since the thread worker is invalid"));
				return;
			}
			SpeechRecognizerSharedPtr->ProcessPCMData(MoveTemp(PCMData), SampleRate, NumOfChannels, bLast);
		});

		return;
	}

	// Resampling to WHISPER_SAMPLE_RATE (16kHz by default) if needed
	if (static_cast<uint32>(SampleRate) != WHISPER_SAMPLE_RATE)
	{
		Audio::FAlignedFloatBuffer ResampledPCMData;

		const Audio::FResamplingParameters ResampleParameters = {
			Audio::EResamplingMethod::FastSinc,
			static_cast<int32>(NumOfChannels),
			static_cast<float>(SampleRate),
			static_cast<float>(WHISPER_SAMPLE_RATE),
			PCMData};

		ResampledPCMData.AddUninitialized(Audio::GetOutputBufferSize(ResampleParameters));
		Audio::FResamplerResults ResampleResults;
		ResampleResults.OutBuffer = &ResampledPCMData;

		if (!Audio::Resample(ResampleParameters, ResampleResults))
		{
			const FString ShortErrorMessage = TEXT("Audio resampling failed");
			const FString LongErrorMessage = FString::Printf(TEXT("%s. sample rate: %f, num of channels: %d"), *ShortErrorMessage, SampleRate, NumOfChannels);
			ReportError(ShortErrorMessage, LongErrorMessage);
			return;
		}

		PCMData = MoveTemp(ResampledPCMData);
	}

	// Reducing the number of channels to 1 if needed
	if (NumOfChannels != 1)
	{
		Audio::TSampleBuffer<float> PCMSampleBuffer(PCMData, NumOfChannels, WHISPER_SAMPLE_RATE);
		{
			PCMSampleBuffer.MixBufferToChannels(1);
		}
		Audio::FAlignedFloatBuffer WaveDataTemp = Audio::FAlignedFloatBuffer(PCMSampleBuffer.GetData(), PCMSampleBuffer.GetNumSamples());
		PCMData = MoveTemp(WaveDataTemp);
	}

	if (bLast)
	{
		PendingAudio.Append(MoveTemp(PCMData));
		const int32 NumOfQueuedSamples = PendingAudio.Num();
		AudioQueue.Enqueue(MoveTemp(PendingAudio));
		bIsFinished.AtomicSet(false);
		UE_LOG(LogRuntimeSpeechRecognizer, Log, TEXT("Enqueued audio data from the pending audio to the queue of the speech recognizer as the last data (num of samples: %d)"), NumOfQueuedSamples);
	}
	else if (RecognitionParameters.StepSizeMs > 0)
	{
		// Calculate the number of samples per step
		const int32 NumOfSamplesPerStep = (1e-3 * RecognitionParameters.StepSizeMs) * WHISPER_SAMPLE_RATE;

		// If pending audio is insufficient to fill the step size, append new data until sufficient
		if (PCMData.Num() + PendingAudio.Num() < NumOfSamplesPerStep)
		{
			PendingAudio.Append(MoveTemp(PCMData));
			UE_LOG(LogRuntimeSpeechRecognizer, Log, TEXT("Pending audio data instead of enqueuing it since it is not enough to fill the step size (pending: %d, num of samples per step: %d)"), PendingAudio.Num(), NumOfSamplesPerStep);
		}
		else
		{
			PendingAudio.Append(MoveTemp(PCMData));
			const int32 NumOfQueuedSamples = PendingAudio.Num();
			AudioQueue.Enqueue(MoveTemp(PendingAudio));
			bIsFinished.AtomicSet(false);
			UE_LOG(LogRuntimeSpeechRecognizer, Log, TEXT("Enqueued audio data from the pending audio to the queue of the speech recognizer (num of samples: %d)"), NumOfQueuedSamples);
		}
	}
	else
	{
		const int32 NumOfQueuedSamples = PCMData.Num();
		AudioQueue.Enqueue(MoveTemp(PCMData));
		bIsFinished.AtomicSet(false);
		UE_LOG(LogRuntimeSpeechRecognizer, Log, TEXT("Enqueued audio data from the pending audio to the queue of the speech recognizer as the last data (num of samples: %d)"), NumOfQueuedSamples);
	}
}

void FSpeechRecognizerThread::ForceProcessPendingAudioData()
{
	if (GetIsStopped())
	{
		const FString ShortErrorMessage = TEXT("Audio processing failed");
		const FString LongErrorMessage = TEXT("The audio data could not be processed to the recognizer since the thread is stopped");
		ReportError(ShortErrorMessage, LongErrorMessage);
		return;
	}

	const int32 NumOfQueuedSamples = PendingAudio.Num();
	AudioQueue.Enqueue(MoveTemp(PendingAudio));
	bIsFinished.AtomicSet(false);
	UE_LOG(LogRuntimeSpeechRecognizer, Log, TEXT("Enqueued audio data from the pending audio to the queue of the speech recognizer as the last data (num of samples: %d)"), NumOfQueuedSamples);
}

bool FSpeechRecognizerThread::Init()
{
	if (GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Cannot initialize the thread since it is stopped"));
		return false;
	}

	if (!WhisperState.WhisperUserData.SpeechRecognizerWeakPtr.IsValid())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("The speech recognizer is not valid"));
		return false;
	}

	return FRunnable::Init();
}

uint32 FSpeechRecognizerThread::Run()
{
	while (!bIsStopped)
	{
		Audio::FAlignedFloatBuffer NewQueuedBuffer;
		while (AudioQueue.Dequeue(NewQueuedBuffer))
		{
			if (whisper_full_parallel(WhisperState.WhisperContext, *WhisperState.WhisperParameters, NewQueuedBuffer.GetData(), NewQueuedBuffer.Num(), 1) != 0)
			{
				UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Failed to process audio data with the size of %d samples to the whisper recognizer"), NewQueuedBuffer.Num());
				return 1;
			}
			UE_LOG(LogRuntimeSpeechRecognizer, Log, TEXT("Processed audio data with the size of %d samples to the whisper recognizer"), NewQueuedBuffer.Num());
		}
			
		if (PendingAudio.Num() == 0 && !bIsFinished)
		{
			bIsFinished.AtomicSet(true);
			AsyncTask(ENamedThreads::GameThread, [this]()
			{
				OnRecognitionFinished.ExecuteIfBound();
			});
		}
	}

	if (GetIsStopped())
	{
		ReleaseMemory();
	}

	return 0;
}

void FSpeechRecognizerThread::Stop()
{
	WhisperState.WhisperUserData = FWhisperSpeechRecognizerUserData();
	bIsFinished.AtomicSet(true);
	bIsStopped.AtomicSet(true);
	FRunnable::Stop();
}

void FSpeechRecognizerThread::Exit()
{
	ReleaseMemory();
	FRunnable::Exit();
}

bool FSpeechRecognizerThread::SetRecognitionParameters(const FSpeechRecognitionParameters& Parameters)
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Cannot set recognition parameters while the thread is running"));
		return false;
	}

	RecognitionParameters = Parameters;
	return true;
}

bool FSpeechRecognizerThread::SetStreamingDefaults()
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Cannot set streaming defaults while the thread is running"));
		return false;
	}

	RecognitionParameters.SetStreamingDefaults();
	return true;
}

bool FSpeechRecognizerThread::SetNonStreamingDefaults()
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Cannot set non-streaming defaults while the thread is running"));
		return false;
	}

	RecognitionParameters.SetNonStreamingDefaults();
	return true;
}

bool FSpeechRecognizerThread::SetNumOfThreads(int32 Value)
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Cannot set the number of threads while the thread is running"));
		return false;
	}

	RecognitionParameters.NumOfThreads = Value;
	return true;
}

bool FSpeechRecognizerThread::SetLanguage(ESpeechRecognizerLanguage Language)
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Cannot set language while the thread is running"));
		return false;
	}

	const USpeechRecognizerSettings* SpeechRecognizerSettings = GetDefault<USpeechRecognizerSettings>();
	if (RecognitionParameters.Language == ESpeechRecognizerLanguage::Auto && SpeechRecognizerSettings->ModelLanguage == ESpeechRecognizerModelLanguage::EnglishOnly)
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Cannot set language to 'auto' when the model is English only"));
		return false;
	}

	RecognitionParameters.Language = Language;
	return true;
}

bool FSpeechRecognizerThread::SetTranslateToEnglish(bool bTranslate)
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Cannot set translation while the thread is running"));
		return false;
	}

	RecognitionParameters.bTranslateToEnglish = bTranslate;
	return true;
}

bool FSpeechRecognizerThread::SetStepSize(int32 Value)
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Unable to set step size while the thread is running"));
		return false;
	}

	RecognitionParameters.StepSizeMs = Value;
	return true;
}

bool FSpeechRecognizerThread::SetNoContext(bool bNoContext)
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Unable to set no context while the thread is running"));
		return false;
	}

	RecognitionParameters.bNoContext = bNoContext;
	return true;
}

bool FSpeechRecognizerThread::SetSingleSegment(bool bSingleSegment)
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Unable to set single segment while the thread is running"));
		return false;
	}

	RecognitionParameters.bSingleSegment = bSingleSegment;
	return true;
}

bool FSpeechRecognizerThread::SetMaxTokens(int32 Value)
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Unable to set max tokens while the thread is running"));
		return false;
	}

	RecognitionParameters.MaxTokens = Value;
	return true;
}

bool FSpeechRecognizerThread::SetSpeedUp(bool bSpeedUp)
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Unable to set speed up while the thread is running"));
		return false;
	}

	RecognitionParameters.bSpeedUp = bSpeedUp;
	return true;
}

bool FSpeechRecognizerThread::SetAudioContextSize(int32 Value)
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Unable to set audio context size while the thread is running"));
		return false;
	}

	RecognitionParameters.AudioContextSize = Value;
	return true;
}

bool FSpeechRecognizerThread::SetTemperatureToIncrease(float Value)
{
	if (!GetIsStopped())
	{
		UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("Unable to set temperature to increase while the thread is running"));
		return false;
	}

	RecognitionParameters.TemperatureToIncrease = Value;
	return true;
}

bool FSpeechRecognizerThread::GetIsStopped() const
{
	return bIsStopped;
}

bool FSpeechRecognizerThread::GetIsFinished() const
{
	return bIsFinished;
}

bool FSpeechRecognizerThread::LoadLanguageModel(uint8*& ModelBulkDataPtr, int64& ModelBulkDataSize)
{
	const USpeechRecognizerSettings* SpeechRecognizerSettings = GetDefault<USpeechRecognizerSettings>();
	if (!SpeechRecognizerSettings)
	{
		const FString ShortErrorMessage = TEXT("Language model loading failed");
		const FString LongErrorMessage = TEXT("Failed to load the default speech recognizer settings");
		ReportError(ShortErrorMessage, LongErrorMessage);
		return false;
	}

	const FString AssetPath = SpeechRecognizerSettings->GetLanguageModelAssetPath();
	UObject* SpeechRecognizerModelObject = StaticLoadObject(USpeechRecognizerModel::StaticClass(), nullptr, *AssetPath);
	if (!SpeechRecognizerModelObject)
	{
		const FString ShortErrorMessage = TEXT("Language model loading failed");
		const FString LongErrorMessage = FString::Printf(TEXT("Failed to load the language model asset '%s'"), *AssetPath);
		ReportError(ShortErrorMessage, LongErrorMessage);
		return false;
	}

	USpeechRecognizerModel* SpeechRecognizerModel = Cast<USpeechRecognizerModel>(SpeechRecognizerModelObject);
	if (!SpeechRecognizerModel)
	{
		const FString ShortErrorMessage = TEXT("Language model retrieval failed");
		const FString LongErrorMessage = FString::Printf(TEXT("Failed to retrieve the language model from the asset '%s'"), *SpeechRecognizerModelObject->GetFullName());
		ReportError(ShortErrorMessage, LongErrorMessage);
		return false;
	}

	ModelBulkDataPtr = nullptr;
	SpeechRecognizerModel->LanguageModelBulkData.GetCopy(reinterpret_cast<void**>(&ModelBulkDataPtr), true);

	ModelBulkDataSize = SpeechRecognizerModel->LanguageModelBulkData.GetBulkDataSize();

	if (!ModelBulkDataPtr)
	{
		const FString ShortErrorMessage = TEXT("Language model buffer retrieval failed");
		const FString LongErrorMessage = FString::Printf(TEXT("Failed to retrieve the buffer data of the language model from the asset '%s'"), *AssetPath);
		ReportError(ShortErrorMessage, LongErrorMessage);
		return false;
	}

	return true;
}

void FSpeechRecognizerThread::ReleaseMemory()
{
	WhisperState.Release();
}

void FSpeechRecognizerThread::ReportError(const FString& ShortErrorMessage, const FString& LongErrorMessage)
{
	UE_LOG(LogRuntimeSpeechRecognizer, Error, TEXT("%s: %s"), *ShortErrorMessage, *LongErrorMessage);
	OnRecognitionError.ExecuteIfBound(ShortErrorMessage, LongErrorMessage);
}
